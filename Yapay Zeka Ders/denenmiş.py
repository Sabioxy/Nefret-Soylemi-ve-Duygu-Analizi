import torch
from transformers import BertTokenizer, BertForSequenceClassification
import customtkinter as ctk
from tkinter import messagebox, filedialog
import speech_recognition as sr
import whisper
import librosa
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv1D,
    MaxPooling1D,
    Flatten,
    Dense,
    Dropout,
    BatchNormalization,
)
from pathlib import Path
import threading
import pickle

# --- MODEL YOLLARI ---
HATE_MODEL_PATH = "hate_speech_model_last.pth"
EMOTION_MODEL_PATH = "best_model1_weights.h5"
SCALER_PATH = "scaler2.pickle"
ENCODER_PATH = "encoder2.pickle"  # Duygu etiketlerini geri dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in (ÅŸu an label_map kullanÄ±lÄ±yor)

# Cihaz yapÄ±landÄ±rmasÄ± (GPU varsa CUDA, yoksa CPU kullan)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Global Model/AraÃ§ YÃ¼kleyicileri (Uygulama baÅŸladÄ±ÄŸÄ±nda bir kez yÃ¼klenir) ---
# Tokenizer ve Whisper modeli bÃ¼yÃ¼k olduklarÄ± ve sÄ±k kullanÄ±ldÄ±klarÄ± iÃ§in globalde yÃ¼klenir
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
whisper_model = whisper.load_model("small")


# --- EÄÄ°TÄ°M KODUNUZDAN ALINAN YARDIMCI FONKSÄ°YONLAR ---
# Bu fonksiyonlar, eÄŸitimde Ã¶zellik Ã§Ä±karma iÃ§in kullandÄ±ÄŸÄ±nÄ±z mantÄ±ÄŸÄ± yansÄ±tÄ±r.


# ZCR (Zero Crossing Rate) hesaplama
def zcr_feature(data, frame_length=2048, hop_length=512):
    zcr = librosa.feature.zero_crossing_rate(
        data, frame_length=frame_length, hop_length=hop_length
    )
    return np.squeeze(zcr)


# RMSE (Root Mean Square Energy) hesaplama
def rmse_feature(data, frame_length=2048, hop_length=512):
    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)
    return np.squeeze(rmse)


# MFCC (Mel-frequency Cepstral Coefficients) hesaplama
# EÄŸitim kodunuzdaki mfcc fonksiyonu n_mfcc'yi belirtmediÄŸi iÃ§in varsayÄ±lan 20 kullanÄ±lÄ±r.
# AyrÄ±ca, mfcc.T'yi dÃ¼zleÅŸtirir (ravel).
def mfcc_feature(data, sr, frame_length=2048, hop_length=512):
    # n_mfcc belirtilmediÄŸi iÃ§in librosa'nÄ±n varsayÄ±lanÄ± olan 20 kullanÄ±lÄ±r.
    # EÄŸer eÄŸitimde farklÄ± bir n_mfcc kullandÄ±ysanÄ±z, burayÄ± gÃ¼ncelleyin.
    mfcc = librosa.feature.mfcc(
        y=data, sr=sr, n_fft=frame_length, hop_length=hop_length
    )
    return np.ravel(mfcc.T)  # EÄŸitimdeki gibi dÃ¼zleÅŸtir (flatten)


# TÃ¼m Ã¶zellikleri birleÅŸtirme (ZCR, RMSE, MFCC)
def extract_all_features(data, sr=22050, frame_length=2048, hop_length=512):
    result = np.array([])
    result = np.hstack(
        (
            result,
            zcr_feature(data, frame_length, hop_length),
            rmse_feature(data, frame_length, hop_length),
            mfcc_feature(data, sr, frame_length, hop_length),  # n_mfcc=20 varsayÄ±lan
        )
    )
    return result


# Duygu tanÄ±ma CNN model mimarisini oluÅŸturan fonksiyon
# BU FONKSÄ°YON KAGGLE'DA EÄÄ°TTÄ°ÄÄ°NÄ°Z MÄ°MARÄ° Ä°LE BÄ°REBÄ°R AYNI OLMALIDIR!
# Conv1D iÃ§in girdi ÅŸekli: (timesteps, features) olmalÄ±dÄ±r.
# Sizin kodunuzda X_train.shape[1] timesteps'Ä± (2376), 1 ise feature sayÄ±sÄ±nÄ± temsil eder.
def create_emotion_model_architecture(input_shape, num_classes):
    model = Sequential(
        [
            Conv1D(
                512,
                kernel_size=5,
                strides=1,
                padding="same",
                activation="relu",
                input_shape=input_shape,
            ),
            BatchNormalization(),
            MaxPooling1D(pool_size=5, strides=2, padding="same"),
            Conv1D(512, kernel_size=5, strides=1, padding="same", activation="relu"),
            BatchNormalization(),
            MaxPooling1D(pool_size=5, strides=2, padding="same"),
            Dropout(0.2),
            Conv1D(256, kernel_size=5, strides=1, padding="same", activation="relu"),
            BatchNormalization(),
            MaxPooling1D(pool_size=5, strides=2, padding="same"),
            Conv1D(256, kernel_size=3, strides=1, padding="same", activation="relu"),
            BatchNormalization(),
            MaxPooling1D(pool_size=5, strides=2, padding="same"),
            Dropout(0.2),
            Conv1D(128, kernel_size=3, strides=1, padding="same", activation="relu"),
            BatchNormalization(),
            MaxPooling1D(pool_size=3, strides=2, padding="same"),
            Dropout(0.2),
            Flatten(),
            Dense(512, activation="relu"),
            BatchNormalization(),
            Dense(num_classes, activation="softmax"),
        ]
    )
    return model


# Duygu etiketleri - BurasÄ± dÃ¼zeltildi!
# Bu sÄ±ralama, gÃ¶zlemlediÄŸiniz yanlÄ±ÅŸ tahminlere gÃ¶re ayarlanmÄ±ÅŸtÄ±r.
label_map = {
    0: "ğŸ˜  Sinirli",
    1: "ğŸ˜¨ KorkmuÅŸ",
    2: "ğŸ˜„ Mutlu",
    3: "ğŸ˜ NÃ¶tr",
    4: "ğŸ˜¢ ÃœzgÃ¼n",
}


class App(ctk.CTk):
    def __init__(self):
        super().__init__()

        # Pencere yapÄ±landÄ±rmasÄ±
        self.title("Duygu ve Metin Analiz UygulamasÄ±")
        self.geometry("800x700")
        self.minsize(750, 650)

        # Tema ayarÄ±
        ctk.set_appearance_mode("System")  # System, Light, Dark
        ctk.set_default_color_theme("dark-blue")

        # Nefret sÃ¶ylemi modelini baÅŸlat (PyTorch BERT modeli)
        self.hate_model = load_hate_model(HATE_MODEL_PATH, 2, device)
        self.is_listening = False

        # --- Duygu Modelini (Keras CNN) ve Ã–n Ä°ÅŸleme AraÃ§larÄ±nÄ± YÃ¼kle ---
        # Ã–NEMLÄ°: Bu deÄŸerler Kaggle'da eÄŸitim sÄ±rasÄ±nda kullanÄ±lanlarla BÄ°REBÄ°R AYNI OLMALIDIR!
        # Sizin verdiÄŸiniz bilgilere gÃ¶re gÃ¼ncellendi:
        # EÄŸitim kodunuzdaki X_train.shape[1] deÄŸeri (2376)
        self.emotion_model_timesteps = 2376
        self.emotion_model_num_classes = (
            5  # Duygu sÄ±nÄ±f sayÄ±sÄ± (label_map ile eÅŸleÅŸmeli)
        )

        # Duygu modeli mimarisini tanÄ±mla
        # Conv1D iÃ§in girdi ÅŸekli: (zaman_adÄ±mÄ±, Ã¶zellik_sayÄ±sÄ±) -> (self.emotion_model_timesteps, 1)
        emotion_model_input_shape = (self.emotion_model_timesteps, 1)  # Yani (2376, 1)
        self.emotion_model = create_emotion_model_architecture(
            emotion_model_input_shape, self.emotion_model_num_classes
        )

        # Duygu modeli aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle
        try:
            self.emotion_model.load_weights(EMOTION_MODEL_PATH)
            print(
                f"Duygu modeli aÄŸÄ±rlÄ±klarÄ± '{EMOTION_MODEL_PATH}' adresinden yÃ¼klendi."
            )
        except Exception as e:
            messagebox.showerror(
                "Model YÃ¼kleme HatasÄ±",
                f"Duygu modeli aÄŸÄ±rlÄ±klarÄ± yÃ¼klenemedi: {str(e)}\n"
                f"LÃ¼tfen '{EMOTION_MODEL_PATH}' dosyasÄ±nÄ±n doÄŸru yolda olduÄŸundan ve model mimarisiyle eÅŸleÅŸtiÄŸinden emin olun.",
            )
            self.emotion_model = (
                None  # YÃ¼klenemezse daha fazla hatayÄ± Ã¶nlemek iÃ§in None olarak ayarla
            )

        # Scaler'Ä± yÃ¼kle (MFCC normalizasyonu iÃ§in)
        try:
            with open(SCALER_PATH, "rb") as f:
                self.scaler = pickle.load(f)
            print(f"Scaler '{SCALER_PATH}' yÃ¼klendi.")
        except Exception as e:
            messagebox.showerror(
                "Scaler YÃ¼kleme HatasÄ±",
                f"Scaler yÃ¼klenemedi: {str(e)}\n"
                f"LÃ¼tfen '{SCALER_PATH}' dosyasÄ±nÄ±n doÄŸru yolda olduÄŸundan emin olun.",
            )
            self.scaler = (
                None  # YÃ¼klenemezse daha fazla hatayÄ± Ã¶nlemek iÃ§in None olarak ayarla
            )

        # Encoder'Ä± yÃ¼kle (duygu etiketi ters dÃ¶nÃ¼ÅŸÃ¼mÃ¼ iÃ§in - isteÄŸe baÄŸlÄ±, ÅŸimdilik label_map kullanÄ±lÄ±yor)
        try:
            with open(ENCODER_PATH, "rb") as f:
                self.encoder = pickle.load(f)
            print(f"Encoder '{ENCODER_PATH}' yÃ¼klendi.")
        except Exception as e:
            messagebox.showwarning(
                "Encoder YÃ¼kleme UyarÄ±sÄ±",
                f"Encoder yÃ¼klenemedi: {str(e)}\n"
                f"LÃ¼tfen '{ENCODER_PATH}' dosyasÄ±nÄ±n doÄŸru yolda olduÄŸundan emin olun.",
            )
            self.encoder = (
                None  # YÃ¼klenemezse daha fazla hatayÄ± Ã¶nlemek iÃ§in None olarak ayarla
            )

        # UI oluÅŸtur
        self.create_widgets()

    def create_widgets(self):
        # BaÅŸlÄ±k Ã§erÃ§evesi
        header_frame = ctk.CTkFrame(self, corner_radius=10)
        header_frame.pack(pady=15, padx=15, fill="x")

        # BaÅŸlÄ±k etiketi
        title_label = ctk.CTkLabel(
            header_frame,
            text="Duygu ve Nefret SÃ¶ylemi Analiz AracÄ±",
            font=ctk.CTkFont(size=22, weight="bold"),
        )
        title_label.pack(pady=(10, 5))

        # Alt baÅŸlÄ±k
        subtitle_label = ctk.CTkLabel(
            header_frame,
            text="Metin veya ses kaydÄ±nÄ± analiz edin",
            font=ctk.CTkFont(size=14),
            text_color="gray70",
        )
        subtitle_label.pack(pady=(0, 10))

        # Girdi Ã§erÃ§evesi
        input_frame = ctk.CTkFrame(self, corner_radius=10)
        input_frame.pack(pady=10, padx=15, fill="x")

        # Metin girdi alanÄ±
        self.input_text = ctk.CTkEntry(
            input_frame,
            placeholder_text="Analiz etmek istediÄŸiniz metni buraya yazÄ±n...",
            height=40,
            font=ctk.CTkFont(size=14),
        )
        self.input_text.pack(pady=10, padx=10, fill="x")

        # DÃ¼ÄŸme Ä±zgarasÄ±
        button_grid = ctk.CTkFrame(self, corner_radius=10)
        button_grid.pack(pady=10, padx=200, fill="x")

        # SatÄ±r 1
        row1 = ctk.CTkFrame(button_grid, fg_color="transparent")
        row1.pack(fill="x", pady=5)

        self.mic_button = ctk.CTkButton(
            row1,
            text="ğŸ¤ Mikrofonla KonuÅŸ",
            command=self.start_listening_thread,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
        )
        self.mic_button.pack(side="left", padx=5)

        self.predict_button = ctk.CTkButton(
            row1,
            text="ğŸ” Metni Analiz Et",
            command=self.analyze_text,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
        )
        self.predict_button.pack(side="left", padx=5)

        # SatÄ±r 2
        row2 = ctk.CTkFrame(button_grid, fg_color="transparent")
        row2.pack(fill="x", pady=5)

        self.file_button = ctk.CTkButton(
            row2,
            text="ğŸ“„ Metin DosyasÄ± AÃ§",
            command=self.analyze_file,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
        )
        self.file_button.pack(side="left", padx=5)

        self.audio_button = ctk.CTkButton(
            row2,
            text="ğŸ”Š Ses DosyasÄ± Analizi",
            command=self.analyze_audio,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
        )
        self.audio_button.pack(side="left", padx=5)

        # SatÄ±r 3
        row3 = ctk.CTkFrame(button_grid, fg_color="transparent")
        row3.pack(fill="x", pady=5)

        self.emotion_button = ctk.CTkButton(
            row3,
            text="ğŸ˜Š Duygu Analizi Yap",
            command=self.analyze_emotion,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
            fg_color="#FF6B6B",
            hover_color="#FF8E8E",
        )
        self.emotion_button.pack(side="left", padx=5)

        self.clear_button = ctk.CTkButton(
            row3,
            text="ğŸ§¹ Temizle",
            command=self.clear_output,
            width=180,
            height=40,
            font=ctk.CTkFont(size=14),
            corner_radius=8,
            fg_color="#4ECDC4",
            hover_color="#88D8C0",
        )
        self.clear_button.pack(side="left", padx=5)

        # Ã‡Ä±ktÄ± Ã§erÃ§evesi
        output_frame = ctk.CTkFrame(self, corner_radius=10)
        output_frame.pack(pady=10, padx=15, fill="both", expand=True)

        # Ã‡Ä±ktÄ± etiketi
        output_label = ctk.CTkLabel(
            output_frame,
            text="Analiz SonuÃ§larÄ±",
            font=ctk.CTkFont(size=16, weight="bold"),
        )
        output_label.pack(pady=(10, 5))

        # KaydÄ±rma Ã§ubuklu Ã§Ä±ktÄ± metin kutusu
        self.output_text = ctk.CTkTextbox(
            output_frame,
            width=750,
            height=300,
            font=ctk.CTkFont(size=13),
            wrap="word",
            activate_scrollbars=True,
        )
        self.output_text.pack(pady=5, padx=10, fill="both", expand=True)

        # Durum Ã§ubuÄŸu
        self.status_var = ctk.StringVar(value="HazÄ±r")
        status_bar = ctk.CTkLabel(
            self,
            textvariable=self.status_var,
            font=ctk.CTkFont(size=12),
            text_color="gray70",
            anchor="w",
        )
        status_bar.pack(side="bottom", fill="x", padx=15, pady=5)

    def start_listening_thread(self):
        if not self.is_listening:
            threading.Thread(target=self.recognize_speech, daemon=True).start()

    def recognize_speech(self):
        self.is_listening = True
        self.mic_button.configure(state="disabled", text="Dinleniyor...")
        self.status_var.set("Mikrofondan dinleniyor... KonuÅŸun")

        r = sr.Recognizer()
        try:
            with sr.Microphone() as source:
                audio = r.listen(source, timeout=5)
            text = r.recognize_google(audio, language="tr-TR")
            self.input_text.delete(0, "end")
            self.input_text.insert(0, text)
            self.analyze_text()
        except sr.WaitTimeoutError:
            self.status_var.set("Mikrofon zaman aÅŸÄ±mÄ±na uÄŸradÄ±")
        except Exception as e:
            self.status_var.set(f"Hata: {str(e)}")
        finally:
            self.is_listening = False
            self.mic_button.configure(state="normal", text="ğŸ¤ Mikrofonla KonuÅŸ")

    def analyze_text(self):
        text = self.input_text.get()
        if not text.strip():
            messagebox.showwarning("UyarÄ±", "LÃ¼tfen analiz edilecek metni girin")
            return

        self.status_var.set("Metin analiz ediliyor...")
        try:
            prediction = predict_hate_speech(text, self.hate_model, tokenizer, device)
            self.output_text.insert("end", f"ğŸ“ Metin: {text}\n")
            self.output_text.insert("end", f"ğŸ” SonuÃ§: {prediction}\n\n")
            self.output_text.see("end")
            self.status_var.set("Analiz tamamlandÄ±")
        except Exception as e:
            messagebox.showerror("Hata", f"Analiz sÄ±rasÄ±nda hata: {str(e)}")
            self.status_var.set("Hata oluÅŸtu")

    def analyze_file(self):
        file_path = filedialog.askopenfilename(
            title="Metin dosyasÄ± seÃ§in", filetypes=(("Metin DosyalarÄ±", "*.txt"),)
        )
        if not file_path:
            return

        self.status_var.set("Dosya okunuyor...")
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

            if not text.strip():
                messagebox.showwarning("UyarÄ±", "Dosya boÅŸ")
                return

            self.input_text.delete(0, "end")
            self.input_text.insert(0, text[:500] + "..." if len(text) > 500 else text)

            prediction = predict_hate_speech(text, self.hate_model, tokenizer, device)
            self.output_text.insert("end", f"ğŸ“‚ Dosya: {Path(file_path).name}\n")
            self.output_text.insert(
                "end", f"ğŸ“ Ä°Ã§erik (kÄ±saltÄ±lmÄ±ÅŸ): {text[:300]}...\n"
            )
            self.output_text.insert("end", f"ğŸ” SonuÃ§: {prediction}\n\n")
            self.output_text.see("end")
            self.status_var.set("Dosya analizi tamamlandÄ±")
        except Exception as e:
            messagebox.showerror("Hata", f"Dosya okunamadÄ±: {str(e)}")
            self.status_var.set("Dosya okuma hatasÄ±")

    def analyze_audio(self):
        file_path = filedialog.askopenfilename(
            title="Ses dosyasÄ± seÃ§in", filetypes=[("Ses DosyalarÄ±", "*.mp3 *.wav")]
        )
        if not file_path:
            return

        self.status_var.set("Ses dosyasÄ± iÅŸleniyor...")
        try:
            result = whisper_model.transcribe(file_path, language="Turkish")
            text = result["text"]

            self.input_text.delete(0, "end")
            self.input_text.insert(0, text)

            prediction = predict_hate_speech(text, self.hate_model, tokenizer, device)
            self.output_text.insert("end", f"ğŸ”Š Ses DosyasÄ±: {Path(file_path).name}\n")
            self.output_text.insert("end", f"ğŸ“ Ã‡Ä±karÄ±lan Metin: {text}\n")
            self.output_text.insert(
                "end", f"ğŸ” Nefret SÃ¶ylemi Analizi: {prediction}\n\n"
            )
            self.output_text.see("end")
            self.status_var.set("Ses analizi tamamlandÄ±")
        except Exception as e:
            messagebox.showerror("Hata", f"Ses analizi baÅŸarÄ±sÄ±z: {str(e)}")
            self.status_var.set("Ses analiz hatasÄ±")

    def analyze_emotion(self):
        if self.emotion_model is None or self.scaler is None:
            messagebox.showerror("Hata", "Model veya scaler yÃ¼klenemedi!")
            return

        file_path = filedialog.askopenfilename(filetypes=[("WAV DosyalarÄ±", "*.wav")])
        if not file_path:
            return

        try:
            self.status_var.set("Duygu analizi iÃ§in ses Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor...")

            # EÄŸitimdeki get_predict_feat fonksiyonuna benzer ÅŸekilde ses yÃ¼kle
            # EÄŸitimde duration=2.5, offset=0.6 kullanÄ±ldÄ±ÄŸÄ± iÃ§in burada da kullanÄ±yoruz.
            # sr=22050 varsayÄ±lan olarak korunuyor.
            y, sr = librosa.load(
                file_path, sr=22050, mono=True, duration=2.5, offset=0.6
            )

            # EÄŸitimdeki extract_features fonksiyonuna benzer ÅŸekilde tÃ¼m Ã¶zellikleri Ã§Ä±kar
            # Bu, ZCR, RMSE ve dÃ¼zleÅŸtirilmiÅŸ (flattened) MFCC'leri iÃ§erir.
            # NOT: mfcc_feature fonksiyonu n_mfcc=20 varsayÄ±lanÄ±nÄ± kullanÄ±r,
            # Ã§Ã¼nkÃ¼ eÄŸitim kodunuzda bu belirtilmemiÅŸti.
            extracted_features = extract_all_features(y, sr)

            # Ã–zellik vektÃ¶rÃ¼nÃ¼ (1D) modelin beklediÄŸi boyuta getir (2376)
            # EÄŸitim kodunuzdaki 'result=np.reshape(result,newshape=(1,2376))' adÄ±mÄ±na karÅŸÄ±lÄ±k gelir.
            # EÄŸer Ã§Ä±karÄ±lan Ã¶zellik vektÃ¶rÃ¼nÃ¼n uzunluÄŸu 2376'dan farklÄ±ysa,
            # bu kÄ±sÄ±m hata verebilir veya yanlÄ±ÅŸ sonuÃ§lar Ã¼retebilir.
            # EÄŸitimde tÃ¼m seslerin 2376 uzunluÄŸunda Ã¶zellik vektÃ¶rÃ¼ Ã¼rettiÄŸi varsayÄ±lÄ±yor.
            if extracted_features.shape[0] != self.emotion_model_timesteps:
                # EÄŸer Ã¶zellik uzunluÄŸu 2376 deÄŸilse, dolgu veya kÄ±rpma yap
                if extracted_features.shape[0] < self.emotion_model_timesteps:
                    processed_features = np.pad(
                        extracted_features,
                        (0, self.emotion_model_timesteps - extracted_features.shape[0]),
                        mode="constant",
                    )
                else:
                    processed_features = extracted_features[
                        : self.emotion_model_timesteps
                    ]
                # print(f"UyarÄ±: Ã‡Ä±karÄ±lan Ã¶zellik uzunluÄŸu {extracted_features.shape[0]}, 2376'ya ayarlandÄ±.")
            else:
                processed_features = extracted_features

            # Scaler iÃ§in (1, 2376) ÅŸekline getir
            # EÄŸitimdeki scaler'Ä±n 1D vektÃ¶rleri beklediÄŸi varsayÄ±lÄ±yor.
            features_for_scaler = processed_features.reshape(1, -1)  # Shape: (1, 2376)

            # NormalleÅŸtirme
            scaled_features = self.scaler.transform(
                features_for_scaler
            )  # Shape: (1, 2376)

            # Model iÃ§in (1, 2376, 1) ÅŸekline getir
            # EÄŸitimdeki 'final_result=np.expand_dims(i_result, axis=2)' adÄ±mÄ±na karÅŸÄ±lÄ±k gelir.
            final_input = np.expand_dims(scaled_features, axis=2)  # Shape: (1, 2376, 1)

            # Tahmin yap
            prediction = self.emotion_model.predict(final_input)

            # --- HATA AYIKLAMA Ã‡IKTILARI ---
            print(f"Raw prediction probabilities: {prediction}")
            predicted_label = np.argmax(prediction, axis=1)[0]
            print(f"Predicted label index: {predicted_label}")
            # --- HATA AYIKLAMA Ã‡IKTILARI SONU ---

            emotion = label_map.get(predicted_label, "Bilinmeyen")

            # SonuÃ§larÄ± gÃ¶ster
            self.output_text.insert("end", f"ğŸµ Ses DosyasÄ±: {Path(file_path).name}\n")
            self.output_text.insert("end", f"ğŸ˜Š Duygu Tahmini: {emotion}\n")
            self.output_text.insert("end", f"ğŸ”¢ OlasÄ±lÄ±klar: {prediction}\n\n")
            self.output_text.see("end")
            self.status_var.set("Duygu analizi tamamlandÄ±")

        except Exception as e:
            messagebox.showerror("Hata", f"Duygu analizi baÅŸarÄ±sÄ±z: {str(e)}")
            self.status_var.set("Duygu analiz hatasÄ±")

    def clear_output(self):
        self.output_text.delete("1.0", "end")
        self.status_var.set("Ã‡Ä±ktÄ± temizlendi")


# --- Model FonksiyonlarÄ± ---
def load_hate_model(model_path, num_labels, device):
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-uncased", num_labels=num_labels, hidden_dropout_prob=0.3
    )
    model.to(device)
    # PyTorch'un FutureWarning'Ä±nÄ± gÃ¶z Ã¶nÃ¼nde bulundurarak weights_only=True ekleyebilirsiniz
    # Ancak eski bir model yÃ¼klÃ¼yorsanÄ±z ve bu hata veriyorsa, kaldÄ±rabilirsiniz.
    # GÃ¼venlik iÃ§in weights_only=True Ã¶nerilir.
    model.load_state_dict(
        torch.load(model_path, map_location=device, weights_only=False)
    )
    model.eval()
    return model


def predict_hate_speech(text, model, tokenizer, device, max_len=128):
    cleaned_text = text.strip()
    encoding = tokenizer.encode_plus(
        cleaned_text,
        add_special_tokens=True,
        max_length=max_len,
        return_token_type_ids=False,
        padding="max_length",
        truncation=True,
        return_attention_mask=True,
        return_tensors="pt",
    )
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.softmax(outputs.logits, dim=1)
        predicted_class = torch.argmax(predictions, dim=1).item()
    return "ğŸš« Nefret SÃ¶ylemi" if predicted_class == 1 else "âœ… Nefret SÃ¶ylemi DeÄŸil"


if __name__ == "__main__":
    app = App()
    app.mainloop()
